{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bOT3u0GZKWc8"
   },
   "source": [
    "# Réseau de neurones: les bases en numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Irgrskx6KWc-"
   },
   "source": [
    "Le but de ce TP est de voir les bases des réseaux de neurones en numpy. Puis de voir comment en pratique on s'en sert avec la librairie Pytorch.\n",
    "\n",
    "Ce TP sera **à rendre** (voir Discord).\n",
    "Il y a n points dans ce TP, qui seront divisé pour obtenir une note sur 4.\n",
    "\n",
    "Ce TP est à faire avec votre **groupe**. Rendez-le avec votre **groupe**.\n",
    "\n",
    "Si vous avez des **questions**, n'hésitez pas à me les poser sur **Discord**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nlmi-culKWdA"
   },
   "source": [
    "## Lecture des données, tokenization et BoW\n",
    "\n",
    "Dans cette section vous devez lire les données (seulement les consensus).\n",
    "\n",
    "Vous devez construire votre tokenizer (et de préférence le sauvegarder).\n",
    "\n",
    "Vous devez transformer votre jeu de données (liste de tweets et labels) en liste de vecteurs BoW + liste d'indice des labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B9Ekg9fkKWdD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Les imports sont préparé ici\n",
    "# n'enlevez pas les % car il permettent le reload de modules ou l'affichage dans le notebook\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle as pkl\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tache 1** 1pt.\n",
    "\n",
    "Importez/réécrivez le code permettant de lire vos données et les transformer en BoW.\n",
    "\n",
    "A la fin de cette cellule, vous devrez avoir trois variables d'instanciées :\n",
    "- X : la liste des vecteurs BoW de vos tweets\n",
    "- Y : la liste des indices des labels de vos tweets (Y[i] doit contenir le label de X[i])\n",
    "- Y_one_hot : la liste des labels sous forme one_hot (c'est similaire à un BoW, vous prenez un vecteur de zeros et mettez un 1 à l'indice du label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading files\n",
    "#Méthode qui renvoie un tableau contenant les lignes des consensus uniquement \n",
    "def reading_file(file):\n",
    "    if platform.system() == 'Windows':\n",
    "        f1 = open(file, \"r\", encoding = \"UTF-8\")\n",
    "    else:\n",
    "        f1 = open(file, \"r\")\n",
    "        \n",
    "    data_ = []\n",
    "\n",
    "    rows = f1.readlines()\n",
    "    for line in rows:   #on itère sur les lignes\n",
    "        \n",
    "            #line.decode(\"utf-8\")\n",
    "        en_tete=line.split(')')[0]                                      #On récupère seulement l'en_tete de chaque ligne\n",
    "        if \"consensus\" in en_tete:                                        #On vérifie que cet en_tete contient le consensus\n",
    "            data_.append(line)                                          #si on a consensus dans la ligne courante on l'ajoute au tableau de renvoie\n",
    "\n",
    "    # closing files\n",
    "    f1.close()\n",
    "\n",
    "    return data_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L'objectif de la fonction est de renvoyer un tableau de labels \n",
    "def labelize (data_,labels):\n",
    "    vectorized_label=[]\n",
    "    for line in data_ :\n",
    "        label=\"\"\n",
    "        for index in range (6,9):\n",
    "            label += line[index]\n",
    "        if label in labels.keys():\n",
    "            vectorized_label.append(labels[label])\n",
    "    \n",
    "    return vectorized_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot (data_):\n",
    "    nb_labels = max(data_)+1\n",
    "    res=[]\n",
    "    for index in data_ :\n",
    "        one_hotted = np.zeros(nb_labels)\n",
    "        one_hotted[index]=1\n",
    "        res.append(one_hotted)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction de pretratement \n",
    "def pretraitement(word_list):\n",
    "        word_list_pretraite = []\n",
    "        \n",
    "        for word in word_list: #on ittere sur les mots\n",
    "            if \"http\" in word :\n",
    "                word_pretraite = '<link>' \n",
    "            elif word.startswith('#'):\n",
    "                word_pretraite = '<hashtag>'\n",
    "            elif word.startswith('@'):\n",
    "                word_pretraite = '<tag>'\n",
    "            else : \n",
    "                word_pretraite = word\n",
    "            \n",
    "            if word_pretraite not in [' ','!','?',',','...']:\n",
    "                word_list_pretraite.append(word_pretraite)\n",
    "            \n",
    "        return word_list_pretraite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#permet de faire un vecteur binaire des mots du tweet par rapport au dico\n",
    "'''def bagOfWords(data_, dico):\n",
    "    boW = []\n",
    "    for vectorize_line in data_:\n",
    "        tab = np.zeros(len(dico))\n",
    "        for index in vectorize_line:\n",
    "            tab[int(index)-1] = 1\n",
    "        boW.append(tab)\n",
    "    return boW\n",
    "'''\n",
    "#permet de faire un vecteur binaire des mots du tweet par rapport au dico\n",
    "def bagOfWords(data_, dico):\n",
    "    boW = np.zeros([len(data_),len(dico)])\n",
    "    for ind,vectorize_line in enumerate(data_):\n",
    "        for index in vectorize_line:\n",
    "            boW[ind,int(index)-1] = 1\n",
    "    return boW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dico_and_tokenization(data_):\n",
    "    i = 1\n",
    "    tokenWord = {}\n",
    "    tokenValues = {}\n",
    "    data_tokenized = []\n",
    "    for line in data_:   #on itère sur les lignes\n",
    "        words_list = line.split(')', 1)[1].split() #array des mots de la ligne\n",
    "        words_list = pretraitement(words_list)\n",
    "        #on rempli les dicos\n",
    "        for j,word in enumerate(words_list):\n",
    "            word = word.lower()\n",
    "            #construction des dicos\n",
    "            if word not in tokenWord:\n",
    "                tokenWord[word] = i\n",
    "                tokenValues[i] = word\n",
    "                i+=1\n",
    "            #tokenisation\n",
    "            words_list[j] = str(tokenWord.get(word))\n",
    "        data_tokenized.append(words_list)\n",
    "    \n",
    "            \n",
    "    return tokenWord, tokenValues, data_tokenized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mettez votre code ici\n",
    "file = \"train_label_temp3.txt\"\n",
    "data_consensus = reading_file(file)\n",
    "dicoWord, dicoValue, data_tokenized = create_dico_and_tokenization(data_consensus)\n",
    "\n",
    "labels = {'pos': 0, 'neg': 1, 'neu': 2, 'irr': 3}\n",
    "#X = bagOfWords(data_tokenized,dicoValue)\n",
    "X = bagOfWords(data_tokenized,dicoValue)\n",
    "Y = labelize(data_consensus,labels)\n",
    "Y_one_hot = one_hot(Y)  # Pour les labels il faut des vecteurs en one_hot, par exemple pour irr : [0,0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tokenization'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-685dc939e18b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# mettez votre code ici\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtokenization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mBagOfWord\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tokenization'"
     ]
    }
   ],
   "source": [
    "# mettez votre code ici\n",
    "\n",
    "from tokenization import *\n",
    "from utils import *\n",
    "from BagOfWord import *\n",
    "\n",
    "corpus = read_corpus(\"/home/boulanger/Teaching/2021-2022/EIT5/train_label_final.txt\", consensus=True)\n",
    "tokenizer = WordTokenizer()\n",
    "\n",
    "\n",
    "labels = {'pos': 0, 'neg': 1, 'neu': 2, 'irr': 3}\n",
    "Y = [] # Pour les labels il faut des vecteurs en one_hot, par exemple pour irr : [0,0,0,1]\n",
    "Y_nonehot = []\n",
    "for i in range(len(corpus)):\n",
    "    lab = re.match(r\"[(].*,(.*),.*[)]\", corpus[i][1])\n",
    "    ap = np.zeros(len(labels.keys()))\n",
    "    ap[labels[lab[1].lower()]] = 1\n",
    "    Y_nonehot.append(labels[lab[1].lower()])\n",
    "    Y.append(ap)\n",
    "\n",
    "\n",
    "tweets = [c[0] for c in corpus]\n",
    "for t in tweets:\n",
    "    tokenizer.add_to_voc(t)\n",
    "\n",
    "    \n",
    "tweets_ids = [tokenizer.words_to_ids(t) for t in tweets]\n",
    "\n",
    "\n",
    "X = [list_to_bow(tw, tokenizer.word2id) for tw in tweets_ids]\n",
    "X_freq = [list_to_bow_freq(tw, tokenizer.word2id) for tw in tweets_ids]\n",
    "\n",
    "\n",
    "print(X[0]) # x sont les tweets transformés en vecteurs BoW\n",
    "print(len(X[0]))\n",
    "print(X_freq[0])\n",
    "print(len(X_freq[0]))\n",
    "print(Y[0]) # y sont les labels transformés en indice\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvZIFXNNKWdV"
   },
   "source": [
    "## Création du modèle\n",
    "\n",
    "Nous allons d'abord créer une couche linéaire.\n",
    "Celle ci comprendra le terme de biais.\n",
    "\n",
    "Rappel de la formule de la couche linéaire: \n",
    "$$\n",
    "\\mathbf{a} = \\mathbf{W}\\mathbf{x}+ \\mathbf{b}\n",
    "$$\n",
    "\n",
    "Notons *n_in* et *n_out* respectivement les dimensions de $\\mathbf{x}$ et $\\mathbf{y}$. \n",
    "\n",
    "**Tache 2** 1pt.\n",
    "- Coder la fonction d'initialisation suivante, l'initialisation est aléatoire Gaussien centrée en 0 avec un écart type 1 / sqrt(n_in). La fonction retourne W et b. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zaHamy9hKWdX"
   },
   "outputs": [],
   "source": [
    "def init(n_in,n_out):\n",
    "    W = np.random.normal(size = (n_in, n_out))/np.sqrt(n_in)\n",
    "    b = np.random.normal(size = (1, n_out))/np.sqrt(n_in) \n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "79A7J5vYKWdZ"
   },
   "source": [
    "- Testez que votre fonction marche avec les dimensions 5 et 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4MGDedaKWda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.56249103 -0.8651999  -0.51480889]\n",
      " [ 0.73858579 -0.39578739 -0.01995374]\n",
      " [-0.14746909 -0.25012218 -0.54619482]\n",
      " [ 0.1292263   0.16963197 -0.31527843]\n",
      " [-0.03465668  0.52529949 -0.4531981 ]] [[-0.33756593  0.44234629 -0.16076642]]\n"
     ]
    }
   ],
   "source": [
    "W, b = init(5, 3)\n",
    "print(W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "euHVEX4iKWdf"
   },
   "source": [
    "**Tache 3** 1pt\n",
    "\n",
    "Implémenter la fonction forward tel que $$a_j = \\sum_{i \\rightarrow j} W_{ij} x_i + b_j$$ \n",
    "\n",
    "\n",
    "où $x_i$ est la valeur du BoW du tweet à l'indice i\n",
    "\n",
    "$W_{ij}$ est la valeur reliant l'indice $i$ vers la catégorie $j$ (c'est la matrice des poids)\n",
    "\n",
    "et $b_j$ est le biais associé à la catégorie $j$\n",
    "\n",
    "Cette fonction dois prendre en entrée un tableau de n vecteur BoW.\n",
    "Vous aurez besoin du produit scalaire de numpy, aussi connu sous le nom dot, et de la transposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "emgTpnkOKWdf"
   },
   "outputs": [],
   "source": [
    "def forward(W,b,X):\n",
    "    \"\"\"\n",
    "        Fait la propagation de l'entrée à travers la couche\n",
    "        :param W: les poids\n",
    "        :param b: le biais\n",
    "        :param X: les input (minibatch_size x n_input)\n",
    "        :type W: ndarray\n",
    "        :type B: ndarray\n",
    "        :type X: ndarray\n",
    "        :return: les valeurs transformés par la couche\n",
    "        :rtype: ndarray\n",
    "    \"\"\"\n",
    "    return np.dot(X, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Testez votre fonction. Pour cela, faites un batch de taille (2, 5) avec la fonction eye de numpy. La sortie de votre fonction doit être de dimension (2, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "[[ 0.22492511 -0.4228536  -0.6755753 ]\n",
      " [ 0.40101986  0.0465589  -0.18072016]]\n"
     ]
    }
   ],
   "source": [
    "Xt = np.eye(2,5)\n",
    "print(Xt)\n",
    "Yt = forward(W, b, Xt)\n",
    "print(Yt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gITyvKlJKWdh"
   },
   "source": [
    "**Tache 4** 1pt\n",
    "\n",
    "Implémenter la fonction softmax stable (on décale les exponentielles grâce à alpha)\n",
    "\n",
    "$$ \\sigma_i = \\frac{\\exp{(a_i - alpha)}}{\\sum_k \\exp{(a_k - alpha)}}$$\n",
    "\n",
    "avec $$ alpha = \\max(a_k, 0) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7LYky4LfKWdm"
   },
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    \"\"\"\n",
    "    Calcule le softmax du batch de sortie de la couche.\n",
    "    :param a: la sortie de la couche (minibatch x n_out)\n",
    "    :type a: ndarray\n",
    "    :return: le softmax\n",
    "    :rtype: ndarray\n",
    "    \"\"\"\n",
    "    alpha = np.amax(a, initial=0, axis=1, keepdims=True)\n",
    "    z = np.exp(a - alpha)\n",
    "    den = np.sum(z, axis=1, keepdims=True)\n",
    "    return np.divide(z,den)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.51824939, 0.27115154, 0.21059907],\n",
       "       [0.44238461, 0.31035559, 0.24725981]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = softmax(Yt)\n",
    "one_hot = np.eye(2,3)\n",
    "sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4BKLEUkNKWdo"
   },
   "source": [
    "**Tache 5** 1 pt\n",
    "\n",
    "Implémenter le calcul du gradient de l'erreur par rapport à la loss à $a_i$:\n",
    "$$\\delta a_i = \\sigma_i - 1_{i=l}$$\n",
    "où $l$ est l'étiquette associée à la donnée courante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JNKTNt3gKWdp"
   },
   "outputs": [],
   "source": [
    "def gradient_out(out, one_hot_batch):\n",
    "    \"\"\"\n",
    "    Calcule le gradient par rapport à la sortie du softmax (on utilise la formule trouvé avec la loss)\n",
    "    :param out: Valeurs du softmax\n",
    "    :type out: ndarray\n",
    "    :param one_hot_batch: la représentation one-hot des labels\n",
    "    :type one_hot_batch: ndarray\n",
    "    :return: le gradient\n",
    "    :rtype: ndarray\n",
    "    \"\"\"\n",
    "    \n",
    "    return out - one_hot_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.48175061,  0.27115154,  0.21059907],\n",
       "       [ 0.44238461, -0.68964441,  0.24725981]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_out(sm, one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4-WxLD1nKWds"
   },
   "source": [
    "**Tache 6** 1pt\n",
    "\n",
    "Implémenter la fonction du calcul de gradient par rapport aux paramètres: $$\\delta W_{ij} = \\delta a_j x_i$$  $$\\delta b_{j} = \\delta a_j$$  (moyenne sur le batch)\n",
    "\n",
    "où $\\delta W_{ij}$ est la composante du gradient associée à l'arête reliant les unités $i$ et $j$, \n",
    "\n",
    "$\\delta b_{j}$ est la composante du gradient associée au bias de l'unité $j$, \n",
    "\n",
    "$\\delta a_j$ est le gradient de l'erreur par rapport à l'unité $j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hDB7Vc7nKWds"
   },
   "outputs": [],
   "source": [
    "def gradient(derror, X, batch_size):\n",
    "    \"\"\"\n",
    "        Calcule le gradient des paramètres\n",
    "        :param derror: le gradient de sortie\n",
    "        :param X: les entrées (minibatch_size x n_input)\n",
    "        :param minibatch_size: la taille du minibatch\n",
    "        :type derror: ndarray\n",
    "        :type minibatch: ndarray\n",
    "        :type minibatch_size: unsigned\n",
    "        :return: le gradient des paramètres\n",
    "        :rtype: ndarray, ndarray\n",
    "    \"\"\"\n",
    "    d_w = np.dot(np.transpose(X), derror)/batch_size\n",
    "    d_b = np.sum(derror, axis=0, keepdims=True)/batch_size\n",
    "        \n",
    "    return d_w, d_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hBSv8LIlKWdu"
   },
   "source": [
    "**Tache 7** 1pt \n",
    "\n",
    "Implémenter la fonction de mise à jour des paramètres $$\\theta = \\theta - \\eta \\delta \\theta$$ où $\\theta$ est un paramètre du modèle et $\\delta \\theta$ la composante du gradient associée à $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RnPLmd-iKWdv"
   },
   "outputs": [],
   "source": [
    "def update(eta, W, b, grad_w, grad_b):\n",
    "    \"\"\"\n",
    "        Mise à jour des paramètres\n",
    "        :param eta: la step-size\n",
    "        :param W: les poids\n",
    "        :param b: le bias\n",
    "        :param grad_w: le gradient des poids\n",
    "        :param grad_b: le gradient du biais\n",
    "        :type eta: float\n",
    "        :type W: ndarray\n",
    "        :type b: ndarray\n",
    "        :type grad_w: ndarray\n",
    "        :type grad_b: ndarray\n",
    "        :return: les paramères mis à jour\n",
    "        :rtype: ndarray, ndarray\n",
    "    \"\"\"\n",
    "    return W - eta*grad_w , b - eta*grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hzcMLu1KKWdx"
   },
   "source": [
    "## Algorithme d'apprentissage \n",
    "\n",
    "On a tout ce qu'il faut pour mettre en oeuvre l'apprentissage d'un modèle simple. Le modèle est simplement une couche neuronale de sortie, sans couche cachée. \n",
    "\n",
    "L'algorithme se déroule en 2 temps, tout d'abord la préparation: \n",
    "- init. du modèle\n",
    "- préparation des données et des variables permettant de stocker l'historique d'apprentissage\n",
    "- init. des paramètres de la SGD\n",
    "- définir le nombre d'époque comme une variable\n",
    "\n",
    "Puis vient la boucle d'apprentissage qui pour chaque époque effectue pour chaque exemple d'apprentissage : \n",
    "- inférence du modèle sur l'exemple d'apprentissage \n",
    "- calcul de la contribution de l'exemple à la  fonction objectif, et également au taux d'erreur de classification\n",
    "- Calcul du gradient de sortie\n",
    "- Mise à jour du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tache 8** 1pt\n",
    "\n",
    "Calculer la log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(y, one_hot_batch):\n",
    "    \"\"\"\n",
    "    Calcul de la loss\n",
    "    :param y: la sortie du softmax\n",
    "    :type y: ndarray\n",
    "    :param one_hot_batch: les labels en one_hot\n",
    "    :type one_hot_batch: ndarray\n",
    "    \"\"\"\n",
    "    return -np.sum(np.log(y) * one_hot_batch) / one_hot_batch.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9136676434619138"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(sm, one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 4)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2]\n",
    "b = [3, 4]\n",
    "c = zip(a, b)\n",
    "list(zip(*c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tache 9** 2pt\n",
    "\n",
    "Programmez la fonction d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(W, b, train_X, train_Y, train_Y_onehot, valid_X, valid_Y, valid_Y_onehot, batch_size=64, epochs=100, eta_0=0.01, eta_dec=0.2):\n",
    "    \n",
    "    rand = random.Random()\n",
    "    rand.seed(a=1)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    f1_batch = []\n",
    "    f1_valid = []\n",
    "    \n",
    "    Ws, bs = [W], [b]\n",
    "    \n",
    "    iteration_counter = 0\n",
    "    for e in range(epochs):\n",
    "        print(f\"epochs : {e}\")\n",
    "        \n",
    "        z = list(zip(train_X, train_Y, train_Y_onehot))\n",
    "        rand.shuffle(z)\n",
    "        \n",
    "        unzip = list(zip(*z))\n",
    "        train_X, train_Y, train_Y_onehot = list(unzip[0]), list(unzip[1]), list(unzip[2])\n",
    "        \n",
    "        for i in range(0, len(train_X), batch_size):            \n",
    "            batch_x = train_X[i: min(i + batch_size, len(train_X))]\n",
    "            batch_y = train_Y[i: min(i + batch_size, len(train_Y))]\n",
    "            batch_y_onehot = train_Y_onehot[i: min(i + batch_size, len(train_Y_onehot))]\n",
    "            \n",
    "            preds = forward(W, b, batch_x)\n",
    "            #print(preds.shape)\n",
    "            \n",
    "            preds = softmax(forward(W, b, batch_x))\n",
    "            \n",
    "            f1_batch.append(f1_score(np.array(batch_y, dtype=int), np.argmax(preds, axis=1), average='micro',\n",
    "                            labels=[i for i in range(4)]))\n",
    "            \n",
    "            losses.append(log_loss(preds, np.array(batch_y_onehot)))\n",
    "            #print(losses[-1])\n",
    "            \n",
    "            grad_w, grad_b = gradient(gradient_out(preds, batch_y_onehot), batch_x, batch_size)\n",
    "            \n",
    "            eta = eta_0 / (1 + eta_dec * iteration_counter)\n",
    "            W, b = update(eta, W, b, grad_w, grad_b)\n",
    "            Ws.append(W)\n",
    "            bs.append(b)\n",
    "            \n",
    "            iteration_counter += 1\n",
    "                        \n",
    "\n",
    "        preds_valid = softmax(forward(W, b, np.array(valid_X)))\n",
    "        \n",
    "        f1_valid.append(f1_score(np.array(valid_Y, dtype=int), np.argmax(preds_valid, axis=1), average='micro',\n",
    "                            labels=[i for i in range(4)]))\n",
    "            \n",
    "    return Ws, bs, f1_batch, f1_valid, losses            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c41050030634>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_Y_nhot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_nonehot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvalid_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1800\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "train_X = X[:1500]\n",
    "train_Y = Y[:1500]\n",
    "train_Y_nhot = Y_nonehot[:1500]\n",
    "\n",
    "valid_X = X[1500:1800]\n",
    "valid_Y = Y[1500:1800]\n",
    "valid_Y_nhot = Y_nonehot[1500:1800]\n",
    "\n",
    "test_X = X[1800:]\n",
    "test_Y = Y[1800:]\n",
    "test_Y_nhot = Y_nonehot[1800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d00a35f540be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_Y_nhot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    }
   ],
   "source": [
    "print(train_X[10])\n",
    "print(train_Y[10])\n",
    "print(train_Y_nhot[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-5efc8ec3f141>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mWs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y_nhot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_Y_nhot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta_0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta_dec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "W, b = init(len(X[0]), 4)\n",
    "Ws, bs, f1_batch, f1_valid,losses = train(W, b, train_X, train_Y_nhot, train_Y, valid_X, valid_Y_nhot, valid_Y, eta_0=0.01, eta_dec=0.001, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iZEhLDilKWd0"
   },
   "source": [
    "**Tache 10** 1 pt\n",
    "\n",
    "Affichez les courbes de loss et de f1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m6m06X92KWd1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f1_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4da983379e60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1 of batches'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1 of validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'best'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#plt.plot(np.linspace(0, len(error_test), 10), error_valid, color='blue')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f1_batch' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot([i/(len(f1_batch)/len(f1_valid)) for i in range(len(f1_batch))],f1_batch, color='red', label='f1 of batches')\n",
    "plt.plot([i for i in range(len(f1_valid))], f1_valid, color='blue', label='f1 of validation')\n",
    "plt.legend(loc='best')\n",
    "#plt.plot(np.linspace(0, len(error_test), 10), error_valid, color='blue')\n",
    "plt.show()\n",
    "#print(np.sum(error_valid[-100:])/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6a3568a23141>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#plt.plot(loss_valid, color='blue')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(losses, color='red')\n",
    "#plt.plot(loss_valid, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2 :  PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de cette section est de répliquer ce que vous avez fait à la main grâce à une librairie spécialisée.\n",
    "\n",
    "Cette librairie est Pytorch.\n",
    "\n",
    "Dans cette librairie vous retrouverez la gestion des poids sous forme de couches dans le sous-module nn.\n",
    "La gestion du gradient est quasiment automatique.\n",
    "\n",
    "Le jeu de données sera toujours les vecteurs BoW obtenus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2a1e0959fb76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_Y_nhot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_nonehot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvalid_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "train_X = X[:1500]\n",
    "train_Y = Y[:1500]\n",
    "train_Y_nhot = Y_nonehot[:1500]\n",
    "\n",
    "valid_X = X[1500:]\n",
    "valid_Y = Y[1500:]\n",
    "valid_Y_nhot = Y_nonehot[1500:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#device = 'cuda' if T.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tache 11** 3 pt\n",
    "\n",
    "Implémentez le même modèle que précédemment.\n",
    "\n",
    "Utilisez les couches Linear et LogSoftmax (les probabilités sont souvent des logs probabilités).\n",
    "Cherchez dans la documentation de torch.nn pour les couches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module): # pour faire un modèle dans pytorch il faut instancier la classe nn.Module\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_dim, out_dim) # Ici se trouve la couche Wx + b\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # Pour pytorch, la plupars des fonction fonctionne avec des logarithmes\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Quelque soit le modèle, il vous faut une fonction forward\n",
    "        # Pour calculer la sortie d'une couche : y = couche(x) avec x un tensor\n",
    "        y = self.linear1(inputs)\n",
    "        preds = self.softmax(y)\n",
    "        return preds    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tache 12** 2pt\n",
    "\n",
    "Complétez la boucle d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    transposed_data = list(zip(*batch))\n",
    "    return T.stack(transposed_data[0], 0), T.stack(transposed_data[1], 0)\n",
    "\n",
    "def train(train_X, train_Y, valid_X, valid_Y, epochs=100, batch_size=64, lr = 1e-3):\n",
    "    \n",
    "    model = Model(len(train_X[0]), 4)\n",
    "    \n",
    "    opti = T.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.NLLLoss() # Pour calculer la crossentropy, il faut calculer la NLLL après un logsofmax\n",
    "    \n",
    "    \n",
    "    ## Transformation des données pour l'entraînement\n",
    "    #trn_X = [T.tensor(x, dtype=T.long) for x in train_X]\n",
    "    #trn_Y = [T.tensor(y, dtype=T.long) for y in train_Y]\n",
    "    \n",
    "    #vld_X = [T.tensor(x, dtype=T.long) for x in valid_X]\n",
    "    #vld_Y = [T.tensor(y, dtype=T.long) for y in valid_Y]\n",
    "    \n",
    "    trn_X = T.tensor(train_X, dtype=T.float)\n",
    "    trn_Y = T.tensor(train_Y, dtype=T.long)\n",
    "    \n",
    "    vld_X = T.tensor(valid_X, dtype=T.float)\n",
    "    vld_Y = T.tensor(valid_Y, dtype=T.long)\n",
    "    \n",
    "    train_set = data.TensorDataset(trn_X, trn_Y)\n",
    "    valid_set = data.TensorDataset(vld_X, vld_Y)\n",
    "    \n",
    "    \n",
    "    ## Creation des loaders\n",
    "    train_sampler = data.BatchSampler(data.RandomSampler(range(len(train_X))), batch_size, False)\n",
    "    valid_sampler = data.BatchSampler(data.SequentialSampler(range(len(valid_X))), len(valid_X), False)\n",
    "    \n",
    "    train_loader = data.DataLoader(train_set, batch_sampler=train_sampler, collate_fn=collate)\n",
    "    valid_loader = data.DataLoader(valid_set, batch_sampler=valid_sampler, collate_fn=collate)\n",
    "    \n",
    "    \n",
    "    losses = []\n",
    "    f1_valid = []\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        model.train() #passe votre modele en phase d'entrainement \n",
    "        \n",
    "        for batch_ndx, (trn_x, trn_y) in enumerate(train_loader):\n",
    "            opti.zero_grad()\n",
    "            \n",
    "            preds = model(trn_x)\n",
    "            loss = criterion(preds, trn_y)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "            \n",
    "        opti.zero_grad()\n",
    "        model.eval()\n",
    "        \n",
    "        for batch_ndx, (vld_x, vld_y) in enumerate(valid_loader):\n",
    "            \n",
    "            preds_val = model(vld_x)\n",
    "            preds = T.argmax(preds_val, dim=1)\n",
    "            f1_valid.append(f1_score(vld_y.to('cpu').numpy(), preds.to('cpu').numpy(), average='micro',\n",
    "                            labels=[i for i in range(4)]))\n",
    "        print(f\"F1 {e}/{epochs}: {f1_valid[-1]}\")\n",
    "    \n",
    "    return model, losses, f1_valid\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-95a1b1046089>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y_nhot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_Y_nhot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    }
   ],
   "source": [
    "model, losses, f1_valid = train(train_X, train_Y_nhot, valid_X, valid_Y_nhot, epochs=200, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-f0a2521e812e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'best'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot([x/(len(losses)/100) for x in range(len(losses))],losses, label=\"loss\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f1_valid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-231bf1dbede8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'best'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f1_valid' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(f1_valid, label='f1')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est super! Vous avez fini. N'hésitez pas à utiliser ce modèle comme base pour votre projet."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "TP1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
