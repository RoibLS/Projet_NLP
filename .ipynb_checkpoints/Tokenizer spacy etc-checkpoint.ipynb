{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97ec9026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Les imports sont préparé ici\n",
    "# n'enlevez pas les % car il permettent le reload de modules ou l'affichage dans le notebook\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle as pkl\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "from sklearn.metrics import f1_score\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6ef3100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading files\n",
    "#Méthode qui renvoie un tableau contenant les lignes des consensus uniquement \n",
    "def readingfile(file):\n",
    "    if platform.system() == 'Windows':\n",
    "        f1 = open(file, \"r\", encoding = \"UTF-8\")\n",
    "    else:\n",
    "        f1 = open(file, \"r\")\n",
    "\n",
    "    data_ = []\n",
    "    data_cons = []\n",
    "\n",
    "    rows = f1.readlines()\n",
    "    for line in rows:   #on itère sur les lignes\n",
    "\n",
    "            #line.decode(\"utf-8\")\n",
    "        en_tete=line.split(')')[0]                                      #On récupère seulement l'en_tete de chaque ligne\n",
    "        if \"consensus\" in en_tete:                                        #On vérifie que cet entete contient le consensus\n",
    "            data_.append(line.split(')')[1])                                          #si on a consensus dans la ligne courante on l'ajoute au tableau de renvoie\n",
    "            data_cons.append(line)\n",
    "            \n",
    "    # closing files\n",
    "    f1.close()\n",
    "\n",
    "    return data_, data_cons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75596190",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"train_label_temp3.txt\"\n",
    "data, datacons = readingfile(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e9c4b4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-ede73c0528e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;34m\"\"\" Cleaning Tweets \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdatacons\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatacons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# removing special characters and numbers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdatacons\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatacons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[^a-z\\s]\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'str'"
     ]
    }
   ],
   "source": [
    "\"\"\" Cleaning Tweets \"\"\"\n",
    "datacons = datacons.str.lower()\n",
    "\n",
    "# removing special characters and numbers\n",
    "datacons = datacons.apply(lambda x : re.sub(\"[^a-z\\s]\",\"\",x) )\n",
    "\n",
    "# removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "datacons = datacons.apply(lambda x : \" \".join(word for word in x.split() if word not in stopwords ))\n",
    "\n",
    "data.head.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15184d50",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'spacy' has no attribute 'Doc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-06dc6aa62b46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mc_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_docs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_doc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'spacy' has no attribute 'Doc'"
     ]
    }
   ],
   "source": [
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "#nlp_fr = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "list_doc = []\n",
    "\n",
    "for i in data :\n",
    "    doc = nlp_en(i)\n",
    "    list_doc.append(doc)\n",
    "\n",
    "    \n",
    "c_doc = Doc.from_docs(list_doc)\n",
    "    \n",
    "    \n",
    "displacy.render(c_doc,style = 'ent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76c9314f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c_doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-399d46db79b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfiltered_sent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdocument\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp_en\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_doc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist_doc\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'c_doc' is not defined"
     ]
    }
   ],
   "source": [
    "#Nettoyage des données --> on enlève les mots inutiles\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "filtered_sent = []\n",
    "\n",
    "document = nlp_en(c_doc)\n",
    "\n",
    "for doc in list_doc :\n",
    "    for word in doc :\n",
    "        if word.is_stop == False:\n",
    "            filtered_sent.append(word)\n",
    "print(\"Filtered Sentence:\",filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92207a81",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c_doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a582d50d1ef0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mc_doc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\\t{8}\".format(\n\u001b[0;32m      3\u001b[0m         \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'c_doc' is not defined"
     ]
    }
   ],
   "source": [
    "for token in c_doc:\n",
    "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\\t{8}\".format(\n",
    "        token.text,\n",
    "        token.idx,\n",
    "        token.lemma_,\n",
    "        token.is_punct,\n",
    "        token.is_space,\n",
    "        token.shape_,\n",
    "        token.pos_,\n",
    "        token.tag_,\n",
    "        token.ent_type_\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d7d938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatisation \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
